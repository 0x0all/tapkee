@article{Belkin2002,
abstract = {Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to...},
author = {Belkin, Mikhail and Niyogi, P.},
editor = {Dietterich, Thomas G and Becker, Suzanna and Ghahramani, Zoubin},
issn = {10495258},
journal = {Science},
pages = {585--591},
publisher = {MIT Press},
title = {{Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering}},
volume = {14},
year = {2002}
}
@inproceedings{Decoste2001,
author = {Decoste, Dennis},
booktitle = {The 8th International Conference on Neural Information Processing ICONIP2001},
title = {{Visualizing Mercer Kernel Feature Spaces Via Kernelized Locally-Linear Embeddings}},
year = {2001}
}
@article{Demmel1999,
abstract = {We investigate several ways to improve the performance of sparse LU factorization with partial pivoting as used to solve unsymmetric linear systems},
author = {Demmel, J. W. and Eisenstat, S. C. and Gilbert, J. R. and Li, X. S. and Liu, J. W. H.},
issn = {08954798},
journal = {SIAM Journal on Matrix Analysis and Applications},
number = {3},
pages = {720},
publisher = {SIAM},
title = {{A Supernodal Approach to Sparse Partial Pivoting}},
volume = {20},
year = {1999}
}
@article{Halko2011Random,
  title={Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions},
  author={Halko, Nathan and Martinsson, P.-G. and Tropp, J. A.},
  journal={SIAM review},
  volume={53},
  number={2},
  pages={217--288},
  year={2011},
  publisher={SIAM}
}
@article{Silva2002,
  title={Global versus local methods in nonlinear dimensionality reduction},
  author={{de Silva}, Vin and Tenenbaum, J. B.},
  journal={Advances in neural information processing systems},
  volume={15},
  pages={705--712},
  year={2002}
}
@article{DeSilva2004,
abstract = {In this paper, we discuss a computationally efficient approximation to the classical multidimensional scaling (MDS) algorithm, called Landmark MDS (LMDS), for use when the number of data points is very large. The first step of the algorithm is to run classical MDS to embed a chosen subset of the data, referred to as the `landmark points', in a low-dimensional space. Each remaining data point can be located within this space given knowledge of its distances to the landmark points. We give an elementary and explicit theoretical analysis of this procedure, and demonstrate with examples that LMDS is effective in practical use.},
author = {{de Silva}, Vin and Tenenbaum, J. B.},
institution = {Stanford University},
journal = {Technology},
keywords = {embedding,feature discovery,online algorithms,unsupervised learn,visualization},
pages = {1--41},
title = {{Sparse multidimensional scaling using landmark points}},
year = {2004}
}
@article{Donoho2003,
abstract = {We describe a method for recovering the underlying parametrization of scattered data (mi) lying on a manifold M embedded in high-dimensional Euclidean space. The method, Hessian-based locally linear embedding, derives from a conceptual framework of local isometry in which the manifold M, viewed as a Riemannian submanifold of the ambient Euclidean space n, is locally isometric to an open, connected subset of Euclidean space d. Because does not have to be convex, this framework is able to handle a significantly wider class of situations than the original ISOMAP algorithm. The theoretical framework revolves around a quadratic form (f) = usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs setlengthoddsidemargin-69pt begindocument dm defined on functions f : M . Here Hf denotes the Hessian of f, and (f) averages the Frobenius norm of the Hessian over M. To define the Hessian, we use orthogonal coordinates on the tangent planes of M. The key observation is that, if M truly is locally isometric to an open, connected subset of d, then (f) has a (d + 1)-dimensional null space consisting of the constant functions and a d-dimensional space of functions spanned by the original isometric coordinates. Hence, the isometric coordinates can be recovered up to a linear isometry. Our method may be viewed as a modification of locally linear embedding and our theoretical framework as a modification of the Laplacian eigenmaps framework, where we substitute a quadratic form based on the Hessian in place of one based on the Laplacian.},
author = {Donoho, David L. and Grimes, C.},
institution = {Department of Statistics, Stanford University, Stanford, CA 94305-4065.},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {10},
pages = {5591--5596},
publisher = {The National Academy of Sciences},
title = {{Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data.}},
volume = {100},
year = {2003}
}
@article{Fredman1987,
abstract = {In this paper we develop a new data structure for implementing heaps (priority queues). Our structure, Fibonacci heaps (abbreviated F-heaps), extends the binomial queues proposed by Vuillemin and studied further by Brown. F-heaps support arbitrary deletion from an n-item heap in O(log n) amortized time and all other standard heap operations in O(1) amortized time. Using F-heaps we are able to obtain improved running times for several network optimization algorithms. In particular, we obtain the following worst-case bounds, where n is the number of vertices and m the number of edges in the problem graph: O(n log n + m) for the single-source shortest path problem with nonnegative edge lengths, improved from O(mlog(m/n+2)n);O(n2log n + nm) for the all-pairs shortest path problem, improved from O(nm log(m/n+2)n);O(n2log n + nm) for the assignment problem (weighted bipartite matching), improved from O(nmlog(m/n+2)n);O(m\&bgr;(m, n)) for the minimum spanning tree problem, improved from O(mlog log(m/n+2)n); where \&bgr;(m, n) = min i \&uharl; log(i)n m/n. Note that \&bgr;(m, n) logn if m n.Of these results, the improved bound for minimum spanning trees is the most striking, although all the results give asymptotic improvements for graphs of appropriate densities.},
author = {Fredman, M. L. and Tarjan, R. E.},
issn = {00045411},
journal = {Journal of the ACM},
number = {3},
pages = {596--615},
publisher = {ACM},
title = {{Fibonacci heaps and their uses in improved network optimization algorithms}},
volume = {34},
year = {1987}
}
@article{He2005,
abstract = {Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is drawn from sampling a probability distribution that has support on or near a submanifold of Euclidean space. In this paper, we propose a novel subspace learning algorithm called neighborhood preserving embedding (NPE). Different from principal component analysis (PCA) which aims at preserving the global Euclidean structure, NPE aims at preserving the local neighborhood structure on the data manifold. Therefore, NPE is less sensitive to outliers than PCA. Also, comparing to the recently proposed manifold learning algorithms such as Isomap and locally linear embedding, NPE is defined everywhere, rather than only on the training data points. Furthermore, NPE may be conducted in the original space or in the reproducing kernel Hilbert space into which data points are mapped. This gives rise to kernel NPE. Several experiments on face database demonstrate the effectiveness of our algorithm},
author = {He, Xiaofei and Cai, D. and Yan, S. and Zhang, H.-J.},
journal = {Tenth IEEE International Conference on Computer Vision ICCV05 Volume 1},
pages = {1208--1213},
publisher = {Ieee},
title = {{Neighborhood preserving embedding}},
volume = {2},
year = {2005}
}
@book{Lehoucq1998,
abstract = {This document is intended to provide a cursory overview of the Implicitly Restarted Arnoldi/Lanczos Method that the software is based upon. The goal is to provide some understanding of the underlying algorithm, expected behavior, and capabilities as well as limitations of the software. Extensive references to the literature on large scale eigenvalue methods and software are given here. Additional information and articles on the algorithmic theory and on applications of ARPACK may be found at...},
author = {Lehoucq, Rich B. and Sorensen, Danny C. and Yang, Chao},
booktitle = {Communication},
publisher = {SIAM},
title = {{Arpack users' guide: Solution of large scale eigenvalue problems with implicitly restarted Arnoldi methods}},
year = {1998}
}
@article{Roweis2000,
abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
author = {Roweis, Sam T. and Saul, Lawrence K.},
institution = {Gatsby Computational Neuroscience Unit, University College London, 17 Queen Square, London WC1N 3AR, UK. roweis@gatsby.ucl.ac.uk},
journal = {Science},
number = {5500},
pages = {2323--2326},
publisher = {AAAS},
title = {{Nonlinear dimensionality reduction by locally linear embedding.}},
volume = {290},
year = {2000}
}
@article{Sonnenburg2010,
abstract = {We have developed a machine learning toolbox, called SHOGUN, which is designed for unified large-scale learning for a broad range of feature types and learning settings. It offers a considerable number of machine learning models such as support vector machines for classification and regression, hidden Markov models, multiple kernel learning, linear discriminant analysis, linear programming machines, and perceptrons. Most of the specific algorithms are able to deal with several different data classes, including dense and sparse vectors and sequences using floating point or discrete data types. We have used this toolbox in several applications from computational biology, some of them coming with no less than 10 million training examples and others with 7 billion test examples. With more than a thousand installations worldwide, SHOGUN is already widely adopted in the machine learning community and beyond. SHOGUN is implemented in C++ and interfaces to MATLAB, R, Octave, Python, and has a stand-alone command line interface. The source code is freely available under the GNU General Public License, Version 3 at http://www.shogun-toolbox.org.},
author = {Sonnenburg, S\"oren and Raetsch, G. and Henschel, S. and Widmer, C. and Behr, J. and Zien, A. and {De Bona}, F. and Binder, A. and Gehl, C. and Franc, V.},
journal = {Journal of Machine Learning Research},
keywords = {learning,statistics \& optimisation,theory \& algorithms},
number = {x},
pages = {1799--1802},
publisher = {MIT Press},
title = {{The SHOGUN Machine Learning Toolbox}},
volume = {11},
year = {2010}
}
@article{Tenenbaum2000,
abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
author = {Tenenbaum, Joshua Brett and {de Silva}, V. and Langford, J. C.},
institution = {Department of Psychology, Stanford University, Stanford, CA 94305, USA. jbt@psych.stanford.edu},
issn = {00368075},
journal = {Science},
keywords = {algorithms,artificial intelligence,face,humans,mathematics,pattern recognition,visual,visual perception},
number = {5500},
pages = {2319--23},
publisher = {AAAS},
title = {{A global geometric framework for nonlinear dimensionality reduction.}},
volume = {290},
year = {2000}
}
@article{Zhang2007,
author = {Zhang, Tianhao and Yang, J. and Zhao, D. and Ge, X.},
issn = {09252312},
journal = {Neurocomputing},
keywords = {alignment,dimensionality reduction,face recognition,linear local tangent space,lltsa,manifold learning},
number = {7-9},
pages = {1547--1553},
title = {{Linear local tangent space alignment and application to face recognition}},
volume = {70},
year = {2007}
}
@article{Zhang2002,
abstract = {Nonlinear manifold learning from unorganized data points is a very challenging unsupervised learning and data visualization problem with a great variety of applications. In this paper we present a new algorithm for manifold learning and nonlinear dimension reduction. Based on a set of unorganized data points sampled with noise from the manifold, we represent the local geometry of the manifold using tangent spaces learned by fitting an affine subspace in a neighborhood of each data point. Those tangent spaces are aligned to give the internal global coordinates of the data points with respect to the underlying manifold by way of a partial eigendecomposition of the neighborhood connection matrix. We present a careful error analysis of our algorithm and show that the reconstruction errors are of second-order accuracy. We illustrate our algorithm using curves and surfaces both in 2D/3D and higher dimensional Euclidean spaces, and 64-by-64 pixel face images with various pose and lighting conditions. We also address several theoretical and algorithmic issues for further research and improvements.},
author = {Zhang, Zhenyue and Zha, H.},
journal = {Journal of Shanghai University English Edition},
number = {4},
pages = {406--424},
publisher = {SIAM},
series = {Lecture Notes in Computer Science},
title = {{Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment}},
volume = {8},
year = {2002}
}
@article{Weyrauch2004,
abstract = {Workshop: Face Processing in Video},
author = {Weyrauch, B. and Heisele, B. and Huang, J. and Blanz, V.},
journal = {2004 Conference on Computer Vision and Pattern Recognition Workshop},
number = {C},
pages = {85--85},
publisher = {Ieee},
title = {{Component-Based Face Recognition with 3D Morphable Models}},
volume = {00},
year = {2004}
}
@article{Coifman_Lafon_2006, 
title={Diffusion maps}, 
volume={21}, 
number={1}, 
journal={Applied and Computational Harmonic Analysis}, 
publisher={Elsevier}, 
author={Coifman, Ronald and Lafon, S.}, 
year={2006}, 
pages={5--30}
}
@article{Zhao2006,
author = {Zhao, Deli},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {lle,ltsa,manifold learning,nonlinear dimensionality reduction},
number = {11},
pages = {2233--2235},
title = {{Formulating LLE using alignment technique}},
volume = {39},
year = {2006}
}
@article{Dhillon2004,
author = {Dhillon, I. S. and Parlett, B. N.},
issn = {00243795},
journal = {Linear Algebra and its Applications},
pages = {1--28},
title = {{Multiple representations to compute orthogonal eigenvectors of symmetric tridiagonal matrices}},
volume = {387},
year = {2004}
}
@article{He2003,
abstract = {Many problems in information processing involve some form of dimensionality reduction. In this paper, we introduce Locality Preserving Projections (LPP). These are linear projective maps that arise by solving a variational problem that optimally preserves the neighborhood structure of the data set. LPP should be seen as an alternative to Principal Component Analysis (PCA) a classical linear technique that projects the data along the directions of maximal variance. When the high dimensional data lies on a low dimensional manifold embedded in the ambient space, the Locality Preserving Projections are obtained by finding the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the manifold. As a result, LPP shares many of the data representation properties of nonlinear techniques such as Laplacian Eigenmaps or Locally Linear Embedding. Yet LPP is linear and more crucially is defined everywhere in ambient space rather than just on the training data points. This is borne out by illustrative examples on some high dimensional data sets.},
author = {He, Xiaofei and Niyogi, P.},
editor = {Thrun, Sebastian and Saul, Lawrence and Sch\"{o}lkopf, Bernhard},
institution = {The University of Chicago},
journal = {Matrix},
number = {December},
pages = {153--160},
publisher = {Citeseer},
title = {{Locality Preserving Projections}},
volume = {16},
year = {2003}
}
@book{Cox2000,
abstract = {The theory of multidimensional scaling arose and grew within the field of the behavioral sciences and now covers several statistical techniques that are widely used in many disciplines. Intended for readers of varying backgrounds, this book comprehensively covers the area while serving as an introduction to the mathematical ideas behind the various techniques of multidimensional scaling.},
author = {Cox, Trevor F. and Cox, M. A. A.},
booktitle = {New York},
isbn = {1584880945},
publisher = {Chapman \& Hall/CRC},
series = {C\&H/CRC Monographs on Statistics \& Applied Probability},
title = {{Multidimensional Scaling, Second Edition}},
volume = {88},
year = {2000}
}
@article{Pe2011,
title={{Scikit-learn: Machine Learning in Python }},
author={Pedregosa, Fabian and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
journal={Journal of Machine Learning Research},
volume={12},
pages={2825--2830},
year={2011}
}
@article{Gashler2011,
abstract = {We present a breadth-oriented collection of cross-platform command-line tools for researchers in machine learning called Waffles. The Waffles tools are designed to offer a broad spectrum of functionality in a manner that is friendly for scripted automation. All functionality is also available in a C++ class library. Waffles is available under the GNU Lesser General Public License.},
author = {Gashler, Michael S.},
journal = {Journal of Machine Learning Research},
number = {July},
pages = {2383--2387},
title = {{Waffles: A Machine Learning Toolkit}},
volume = {12},
year = {2011}
}
@article{Zito2009,
abstract = {Modular toolkit for Data Processing (MDP) is a data processing framework written in Python. From the user's perspective, MDP is a collection of supervised and unsupervised learning algorithms and other data processing units that can be combined into data processing sequences and more complex feed-forward network architectures. Computations are performed efficiently in terms of speed and memory requirements. From the scientific developer's perspective, MDP is a modular framework, which can easily be expanded. The implementation of new algorithms is easy and intuitive. The new implemented units are then automatically integrated with the rest of the library. MDP has been written in the context of theoretical research in neuroscience, but it has been designed to be helpful in any context where trainable data processing algorithms are used. Its simplicity on the user's side, the variety of readily available algorithms, and the reusability of the implemented units make it also a useful educational tool.},
author = {Zito, T. and Wilbert, N. and Wiskott, L. and Berkes, P.},
institution = {Bernstein Center for Computational Neuroscience Berlin, Germany.},
journal = {Frontiers in neuroinformatics},
keywords = {computational neuroscience,machine learning,modular toolkit data processing,python},
number = {January},
pages = {7},
publisher = {Frontiers Research Foundation},
title = {{Modular Toolkit for Data Processing (MDP): A Python Data Processing Framework}},
volume = {2},
year = {2009}
}
@other{Vincent,
author = {Vincent, P. and Bengio, Y. and Chapados, N. and Delalleau, O.},
title = {{PLearn} high-performance machine learning library},
url = {http://plearn.berlios.de/}
}
@other{Lehoucq,
author = {Lehoucq, Rich and Maschhoff, K. and Sorensen, D. and Yang C.},
title = {{ARPACK} numerical software library},
url = {http://www.caam.rice.edu/software/ARPACK/}
}
@other{SherryLi,
author = {{Sherry Li}, X. and Demmel, J. and Gilbert, J. and Grigori, L. and Shao, M. and Yamazaki, I.},
title = {{SuperLU} numerical software library},
url = {http://crd-legacy.lbl.gov/~xiaoye/SuperLU/}
}
@report{VanDerMaaten2007, 
title={{An Introduction to Dimensionality Reduction Using MATLAB}},
volume={1201}, 
number={July}, 
publisher={Maastricht, Universiteit}, 
author={{van der Maaten}, Laurens}, 
year={2007}
}
@article{Beygelzimer2006, 
title={Cover trees for nearest neighbor}, 
volume={1},
journal={Proceedings of the 23rd international conference on Machine learning ICML 06}, 
publisher={ACM Press}, 
author={Beygelzimer, Alina and Kakade, S. and Langford, J.}, 
year={2006}, 
pages={97--104}
}
@article{Agrafiotis2003, 
title={Stochastic Proximity Embedding}, 
journal={Journal of Computational Chemistry},
author={Agrafiotis, Dimitris K.},
year={2003},
volume={24},
number={10},
pages={1215--1221}
}
@article{Sarwar2000, 
title={Application of dimensionality reduction in recommender systems - a case study}, 
journal={Proc. of the ACM WebKDD Workshop},
author={Sarwar, B. and Karypis, G. and Konstan, J. and Riedl, J.}, 
year={2000},
}
@article{Scholkopf1997,
  title={Kernel principal component analysis},
  author={Sch{\"o}lkopf, Bernhard and Smola, A. and M{\"u}ller, K.-R.},
  journal={Artificial Neural Networks, ICANN'97},
  pages={583--588},
  year={1997},
  publisher={Springer}
}
@article{Pearson1901,
  title={On lines and planes of closest fit to systems of points in space},
  author={Pearson, Karl},
  journal={The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume={2},
  number={11},
  pages={559--572},
  year={1901},
  publisher={Taylor \& Francis}
}
@MISC{eigenweb,
  author = {Ga\"{e}l Guennebaud and Beno\^{i}t J. and others},
  title = {Eigen v3},
  howpublished = {http://eigen.tuxfamily.org},
  year = {2010}
 }
@article{Weka,
  title={The {WEKA} data mining software: an update},
  author={Hall, Mark and Frank, E. and Holmes, G. and Pfahringer, B. and Reutemann, P. and Witten, I. H.},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={11},
  number={1},
  pages={10--18},
  year={2009},
  publisher={ACM}
}
@Unpublished{Mahout,
  key = {Apache Mahout},
  title = {Apache {Mahout}, http://mahout.apache.org},	
  url = {http://mahout.apache.org}
}
@article{Halko2011,
  title={An algorithm for the principal component analysis of large data sets},
  author={Halko, Nathan and Martinsson, P.-G. and Shkolnisky, Y. and Tygert, M.},
  journal={SIAM Journal on Scientific Computing},
  volume={33},
  number={5},
  pages={2580--2594},
  year={2011},
  publisher={SIAM}
}
@inproceedings{Kaski1998,
  title={Dimensionality reduction by random mapping: Fast similarity computation for clustering},
  author={Kaski, Samuel},
  booktitle={Neural Networks Proceedings, 1998. IEEE World Congress on Computational Intelligence},
  volume={1},
  pages={413--418},
  year={1998},
  organization={IEEE}
}
@article{Spearman1904,
  title={General Intelligence, Objectively Determined and Measured},
  author={Spearman, Charles},
  journal={The American Journal of Psychology},
  pages={201--292},
  year={1904},
  publisher={JSTOR}
}
@article{BarnesHut,
  title={{Barnes-Hut-SNE}},
  author={{van der Maaten}, Laurens},
  journal={arXiv preprint arXiv:1301.3342},
  year={2013}
}
@article{tSNE,
  title={Visualizing data using {t-SNE}},
  author={{van der Maaten}, Laurens and Hinton, G.},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={2579-2605},
  pages={85},
  year={2008}
}
@inproceedings{SonFra10,
 author =        {Sonnenburg, S\"oren and Franc, V.},
 title =         {{COFFIN:} A Computational Framework for Linear {SVMs}},
 booktitle =	 {Proceedings of the 27nd International Machine Learning Conference},
 year =			 {2010},
 pdf =  {http://sonnenburgs.de/soeren/publications/SonFra10.pdf},
 dataset = {http://sonnenburgs.de/soeren/coffin},
}
@inproceedings{MitCbcl,
  title={Component-based face recognition with {3D} morphable models},
  author={Huang, Jennifer and Heisele, B. and Blanz, V.},
  booktitle={Audio and Video-Based Biometric Person Authentication},
  pages={1055-1055},
  year={2003},
  %organization={Springer}
}
